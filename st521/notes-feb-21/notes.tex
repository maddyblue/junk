\documentclass{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{amsmath}
\begin{document}

Notes - 21 feb

$X_n$ branching process. $\{P_k\}$ offspring distribution. $\phi(s) = E(s^{X_1}) = \Sigma_{k=0}^{\infty}P_k s^k$. We are interested in $\phi_n(s)$ = prob. gen. function of $X_n$.

Fixed point theory (aside, not in notes). Solve the problem given a function g, determine x such that g(x) = x [fixed point problem, x = fixed point].

Define f(s) = g(s)-s then f(x) = 0 iff g(x) = x. root problem connected to fixed point problems.

Fixed point iteration. Given an initial guess $X_0$, let $X_1 = g(X_0), X_2 = g(X_1), \dots$. Under certain conditions (Banach, \dots), $X_n \rightarrow X$.

Example 2.5.3 - Suppose $0 \le p \le 1$ and pmf for the offspring is $\{q p^k\}_{k \ge 0}, q = 1-p$. The prob. gen. function is $\phi(s) = q(1-ps)^{-1}$. $\Sigma_{k=0}^{\infty} q p^k s^k = q \Sigma_{k=0}^{\infty} (ps)^k, |ps| < 1$. Each family size is one less than a geometric variable. By induction, $\phi_n(s) = \{ \frac{n-(n-1)s}{n + 1 - ns} for p = q = 1/2, \frac{q(p^n - q^n - ps (p^{n-1}-q^{n-1}))}{p^{n+1}-q^{n+1}-ps(p^n-q^n)} for p \ne q$.

We have \{ultimate extinction\} = $U_n \{X_n = 0\}$. Moreover, $A_n = \{X_n=0\},$ satisfies $A_n < A_{n+1} < \dots$. Thus, $A = U_{n=1}^{\infty} A_n = \lim_n A_n$. $P(A) = \lim_{n \rightarrow \infty} P(A_n)$. $P(X_n=0) \rightarrow_{n \rightarrow \infty} P(ultimate extinction) = \{ 1 for p \le q, q/p for p > q$.

Extinction occurs with probability 1 if $\mu = E(X_1) = p/q = \frac{1}{(\frac{q}{p})}$ satisfies $\mu < 1$. Makes sense: if $E(X_1) < 1$ then $X_n = 0$ at some point.

Theorem 2.5.8 - let $\phi$ be the prob. gen. function of the offspring. Then P(ultimate extinction) $= \lim_{n \rightarrow \infty} P(X_n = 0) = \eta$, where $\eta$ is the smallest nonnegative fixed point of the equation (2.5.15) $\phi(s) = s$. (1) If $|\phi^\prime (1)| < 1, \eta > 1$. (2) If $|\phi^\prime(1)| > 1, \eta < 1$. (3) If $|\phi^\prime(1)| = 1, \eta = 1$ as long as the offspring distribution has positivie variance.

Proof: Let $\eta_n = P(X_n=0)$. Theorem 2.5.7 implies (2.5.16) $\eta_n = \phi_n(0) = \phi(\phi_{n-1}(0)) = \phi(\eta_{n-1})$ (fixed point iteration). From the discussion above, $\eta_n \uparrow \eta$. Since $\phi$ is continuous, take the limit in (2.5.16) gives $\eta = \phi(\eta), \eta_n = \phi(\eta_{n-1}), \lim_{n \rightarrow \infty} \eta_n = \eta = \phi(\lim_{n \rightarrow \infty} \eta_{n-1}) = \phi(\eta)$.

Suppose $\psi$ is a nonnegative root of $s = \phi(s)$. We show $\eta \le \psi$. $\phi$ is nondecreasing on [0, 1]. $\phi(s) = \Sigma_{k=0}^{\infty} P_k s^k$. $\eta_1 = \phi(0) \le \phi(\psi) = \psi, \eta_2 = \phi(\eta_1) \le \phi(\psi) = \psi, \dots, \eta \le \psi$.

To proove the rest, we use the fact that $\phi$ is convex. $\phi''(s) = E(X_1(X_1-1)s^{X_1-2}) \ge 0$ if $s \ge 0$. $\phi$ is convex and nondecreasing on [0, 1] and $\phi(1) = 1$. The two curves $Y=s$ and $Y=\phi(s)$ can have two intersection points in [0, 1], $\eta$ and 1. If $|\phi'(1)| < 1, \eta = 1$. If $|\phi'(1)| > 1, \eta \ne 1$. (Consult graphs in notes.)

Example 2.5.4 - An individual has a disease. Transmits the disease to some fraction of people that are contacted. Assume: a person is infectious and has the disease for a brief period. Each infected person contacts a number of people with a Poisson distribution with mean 10, and each contact has probability $p$ of being infected. Model the infected population $X_n$. For $p = .2$, what is the prob. that the disease will die out. If $p < .1$, the disease will die out (ultimate extinction). prob gen func of a Poisson process with mean $\mu$ is $e^{\mu(s-1)}$, so we solve $s = e^{.2*10(s-1)} = e^{2(s-1)} \Rightarrow s \approx .2$.

Example 2.5.5 - Suppose $p_0 = 1/5, p_1 = 1/2, p_2 = 3/10, p_i = 0, i \ge 3. \mu = 1/2 + 2 * 3/10 = 11/10 > 1. \phi(s) = 1/5 + 1/2 s + 3/10 s^2$. Verify $\phi'(1) = \mu$. Prob of ultimate extinction: $1/5 + t/2 + \frac{3t^2}{10} = t \Rightarrow \eta = 2/3, \eta = 1$ (2/3 was circled, 1 not circled). See ex 2.5.6 in notes.

Chapter 3 - Long time behavior for Markov Chains

$\S$ 3.1 - Classification of States

Viewpoint here: think of the Markov chain as describing the motion of a particle in the state space. Does a particle return to its starting state within some time? It suffices to consider the distribution of the length of time until the particle returns the first time. Let $X_n$ be a Markov chain with state space $S$.

Definition 3.1.1 - A state $i$ is persistent or recurrent if $P(X_n=i $ for some $ n \ge 1| X_0=i)=1$. If $P(X_n=i $ for some $ n\ge 1|X_0=i) < 1$, $i$ is transient.

\end{document}
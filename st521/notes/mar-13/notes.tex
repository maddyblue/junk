\documentclass{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{amsmath}
\begin{document}

Notes - 13 Mar

The assumption that the chain is irreducible is important.

Example 3.3.3 - Consider the genotype example, ex 3.1.4: P = (1 0 0 \& 1/4 1/2 1/4 \& 0 0 1). The chain is not irreducible. We try to find a stationary distribution anyway. $\Pi = \Pi P$ becomes $\Pi_0 \Pi_1 \Pi_2)$ (1 0 0 \& 1/4 1/2 1/4 \& 0 0 1)$ = (\Pi_0 \Pi_1 \Pi_2)$. 1st: $\Pi_0 + 1/4 \Pi_1 = \Pi_0 \Rightarrow \Pi_1 = 0$. 2nd: $0 = 0$. 3rd: $\Pi_2 = \Pi_2$. Any distribution of the form $\Pi = (\alpha, 0, 1-\alpha, 0 \le \alpha \le 1$ is a stationary distribution.

In the finite state space case, we later prove:
Theorem 3.3.2 - If the state space has $r$ states, then $\Pi = \Pi P$ has at most $r-1$ linearly independent equations. Including the equation $\Sigma_{j \in S} \Pi_j = 1$, we obtain at most $r$ linearly independent equations. There is always atleast one solution. If the chain is not irreducible there may be more than one solution. The situation with an infinite state space case is more complicated.

Example 3.3.4 - Consider the gambler's ruin model, ex 3.1.12, where A has a backer and B is infinitely rich. Assume $r_i = 0$, and $q_i = p_i = 1/2$, and A starts with \$1. P = (1/2 1/2 0 ... \& 1/2 0 1/2 0 ... \& 0 1/2 0 1/2 0 ... \& ...). $\Pi = \Pi P$. 1st: $1/2 \Pi_0 + 1/2 \Pi_1 = \Pi_0 \Rightarrow \Pi_1 = \Pi_0$. 2nd: $1/2 \Pi_0 + 1/2 \Pi_2 = \Pi_1 = \Pi_0 \Rightarrow \Pi_2 = \Pi_0$. Continuing, we find that IF the distribution exists, then $\Pi_n = \Pi_0$ for all $n$. $\Pi = (\Pi_0, \Pi_0, \dots) \Rightarrow \Sigma \Pi_0 = 1$, which is impossible. So there are no such stationary distributions.

Suppose $r_i = 0$, but $P_0 = P_1 = P_2 = \dots = p < 1/2$. P = (1-p p 0 ... \& 1-p 0 p 0 ... \& 0 1-p 0 p 0 ... \& ...). $\Pi P = \Pi$. 1st: $(1-p)\Pi_0 + (1-p)\Pi_1 = \Pi_0 \Rightarrow \Pi_1 = \frac{p}{1-p} \Pi_0$. 2nd: $\Pi_2 = (\frac{p}{1-p})^2 \Pi_0, \dots, \Pi_n = (\frac{p}{1-p})^n \Pi_0, n \ge 1, \Sigma \Pi_i = 1 \Rightarrow 1 = \Pi_0 * \Sigma_{n=0}^\infty (\frac{p}{1-p})^n = \Pi_0 \frac{1-p}{1-2p}, \Pi_n = \frac{1-2p}{1-p} (\frac{p}{1-p})^n, n \ge 0$ for the stationary distribution.

In both cases, the chain is irreducible and recurrent. In the first example, it is null, and in the second it is positive.

Theorem 3.3.3 - An irreducible chain has the stationary distribution $\Pi$ iff all the states are positive recurrent. In this case there is a unique stationary distribution. The solution is $\Pi_i = \frac{1}{\mu_i}, i \in S$, where $\mu_i$ is the mean recurrence time. The proof has several parts.

Definition 3.3.3 - Fix state $k$. Define $\rho_i(k) =$ mean number if visits to state $i$ between two successive visits to state $k$. $T_k =$ min$\{n \ge 1: X_n=k | X_0=k\} =$ time of first return to state $k$. $N_i = \Sigma_{n=1}^\infty 1_{\{X_n=i\} \cap \{T_k \ge n\}}$ (count 1s). $\{X_n=i\} \cap \{T_k \ge n\}$ is the event where $X_n$ is in state $i$ and the time to return to state $k$ is larger than $n$. $N_i$ counts the number of visits to state $i$ between successive visits to state $k$. Hence, $\rho_i(k) = E(N_i|X_0=k)$. Now $N_k = 1$, so $\rho_k(k) = 1$. We also have (3.3.2) $\rho_i(k) = \Sigma_{n=1}^\infty P(X_n=i, T_k \ge n|X_0=k)$. $\rho(k) = (\rho_1(k) \rho_2(k) \dots)$.

Theorem 3.3.4 - The mean recurrence time $\mu_k$ satisfies (3.3.3) $\mu_k = \Sigma_{i \in S} \rho_i(k)$. Proof: The time between visits to state $k$ must be spent in some state. $T_k = \Sigma_{i \in S} N_i$. Take expectations.

Theorem 3.3.5 - For any state $k$ of an irreducible, recurrent chain, $\rho(k)$ satisfies $\rho_i(k) < \infty$ for all $i$ and $\rho(k) = \rho(k) P$. Proof: first, $\rho_i(k) < \infty$. Set $l_{ki}(n) = P(X_n=i, T_k \ge n|X_0=k) =$ probability that the chain reaches state $i$ in $n$ steps, with no intermediate returns to state $k$. Observation: the first return time to $k$ is $m + n$ if (1) $X_m = i$, (2) there is no return to $k$ before $m$ steps, and (3) the next visit to state $k$ occurs after $n$ more steps. This implies $f_{kk}(m+n) \ge l_{ki}(m) f_{ik}(n)$. Since the chain is irreducible, there is an $n$ with $f_{ik}(n) > 0$. Using this $n$, $l_{ki}(m) \le \frac{f_{kk}(m+n)}{f_{ik}(n)}$. We have $\rho_i(k) = \Sigma_{n=1}^\infty l_{ki}(m)$ (important, exercise: explain) $\le \frac{1}{f_{ik}(n)} \Sigma_{m=1}^\infty f_{kk}(m+n) \le \frac{1}{f_{ik}(n)} < \infty$. To prove the second claim, start with $\rho_i(k) = \Sigma_{n=1}^\infty l_{ki}(n)$. Now $l_{ki}(n) = P_{ki}, n \ge 2, l_{ki}(n) = \Sigma_{j \in S, j \ne k} P(X_n=i|X_{n-1}=j, T_k \ge n|X_0=k) = \Sigma_{j \in S, j \ne k} l_{kj}(n-1)P_{ji}$. Summing in $n$, $\rho_j(k) = P_{ki} + \Sigma_{j \in S, j \ne k} (\Sigma_{n \ge 2} l_{kj}(n-1)) P_{ji} = \rho_k(k) P_{ki} + \Sigma_{j \in S, j \ne k} \rho_j(k) P_{ji}$.

Theorem 3.3.6 - Every positive recurrent irreducible chain has a stationary distribution. Proof: We just proved $\rho(k) = \rho(k) P$. The components of $\rho(k)$ are nonnegative and sum to $\mu_k$. If $\mu_k < \infty, \Pi: \Pi_i = \rho_i(k) / \mu_k \Rightarrow \Pi = \Pi P, \Pi$ is a p.m.f.

\end{document}
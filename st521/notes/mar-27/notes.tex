\documentclass{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{amsmath}
\begin{document}

Notes - 27 Mar

Theorem 3.3.8 - Let $s \in S$ be a state of an irreducible chain. The chain is transient $\iff$ there is a nonzero solution$\{Y_i, i \in S\}$ of (3.3.5) $Y_i = \Sigma_{j \in S, j \ne s} P_{ij} Y_j, i \ne s$, with $|Y_i| \le 1$ for all $i$.

Proof - The chain is transient iff s is transient. Suppose s is transient. Define (3.3.6) $\tau_i(n) = P($no visit to s in the first n steps$|X_0=i) = P(X_m \ne s, 1 \le m \le n | X_0=i)$. $\tau_i(1) = \Sigma_{j \ne s} P_{ij}$, which is $X_1 \ne s$. $\tau_i(n+1) = \Sigma_{j \ne s} P_{ij} \tau_i(n)$. Furthermore, $\tau_i(n) \ge \tau_i(n+1), \tau_i = \lim_{n \rightarrow \infty} \tau_i(n) = P($no visit to s$|X_0=i) = 1 - f_{is}$. Exercise: $\tau_i$ satisfies (3.3.5). Also $\tau_i > 0$ for some $i$. Otherwise $f_{is} = 1$ for all $i \ne s$. This implies (condition on $X_1$) $f_{ss} = P_{ss} [X_1 = s] + \Sigma_{i \ne s} P_{si} f_{is} [X_1 \ne s] = \Sigma_i P_{si} = 1$ which contradicts the transiency of s. Let $Y$ satisfy (3.3.5) with $|Y_i| \le 1$. $|Y_i| \le \Sigma_{j \ne s} P_{ij} |Y_j| \le \Sigma_{j \ne s} P_{ij} = \tau_i(1)$. Going back to (3.3.5) $|Y_i| \le \Sigma_{j \ne s} P_{ij} \tau_j(1) = \tau_i(2) \dots$. So, $|Y_i| \le \tau_i(n)$ for all $n$. Exercise: Let $n \rightarrow \infty$ to show that $\tau_i = \lim_{n \rightarrow \infty} \tau_i(n) > 0$ for some $i$ ($Y_i \ne 0$ for some $i$ by assumption) which implies $s$ is transient.

Theorem 3.3.9 - An irreducible chain is recurrent iff the only bounded solution of (3.3.5) is the zero solution.

Example 3.3.7 - Gambler's Ruin ex 3.1.2 - P = (q p 0 \& q 0 p 0 \& 0 q 0 p 0 \& \dots), $q + p = 1$. Set $\gamma = \frac{p}{q}$. (1) If $q < p (\gamma > 1)$, choose $s = 0$ to test thm 3.3.8. (3.3.5) read: $Y_0 = P_{01} Y_1 = p Y_1, Y_1 = P_{02} Y_2 = p Y_2, Y_2 = q Y_1 + p Y_3 $(this is what was written on the board, should the $+$ be there?), \dots. Exercise: if $Y_j = 1 - \gamma^{-j}$, then $Y$ solves the equations and the chain is transient. (2) We can solve $\Pi = \Pi P$ to find a stationary solution with $\Pi_j = \gamma^j (1 - \gamma) \iff q > p$. The chain is positive recurrent $\iff q > p$.

Example 3.3.8 - Consider discrete queuing (ex 2.1.10). Customers arrive at a service place and take a place in a line (queue). In each period of time, 1 customer is served and a random number arrive. $C_n = $number of customers that arrive during nth period. $P(C_n = k) = a_k$, where $a =$ p.m.f. $X_n =$ number of customers waiting in line at time $n$. $X_{n+1} =$ max$\{X_n - 1, 0\} + C_n$. P = ( a0 a1 a2 \dots \& a0 a1 a2 \dots \& 0 a0 a1 a2 \dots \& \dots). $\Pi = \Pi P, \Pi_0 = \Pi_0 a_0 + \Pi_1 a_0, \Pi_1 = \Pi_0 a_1 + \Pi_1 a_1 + \Pi_2 a_2, \Pi_2 = \Pi_0 a_2 + \Pi_1 a_2 + \Pi_2 a_1 + \Pi_3 a_0, \dots$. (3.3.7) $\Pi_i = a_0 \Pi_{i+1} + \Sigma_{j = 1}^{i+1} \Pi_j a_{i+1-j}$. We use generating functions: $\Pi(t) = \Sigma_{i = 0}^\infty \Pi_i t^i$. We multiply (3.3.7) by $t^i$ and sum. (3.3.8) $\Pi(t) = \Pi_0 * \Sigma_{i=0}^\infty a_i t^i + \Sigma_{i=0}^\infty \Sigma_{j=1}^{i+1}\Pi_j a_{i+1-j} t^i. 1 \le j \le i + 1 \Rightarrow i \ge j - 1, j \ge 1, (A(t) =$ p.g.f. for a$) A(t) = \Sigma_{i=0}^\infty a_i t^i$, the right-hand side of (3.3.8) is $\Pi_0 A(t) + \Sigma_{j=1}^\infty \Pi_j t^{j-1} * \Sigma_{i=j-1}^\infty a_{i-j+1}t^{i-j+1} = \Pi_0 A(t) = t^{-1} (\Sigma_{j=1}^\infty \Pi_j t^j) A(t) = \Pi_0 A(t) + t^{-1} (\Pi(t) - \Pi_0) A(t)$ or $\Pi(t) = \Pi_0 A(t)(1-t^{-1} + t^{-1}\Pi(t)A(t) \Rightarrow (3.3.9) \Pi(t) = \frac{\Pi_0 A(t)}{1 - \frac{1 - A(t)}{1-t}}$. The question is: when is it possible to specify $\Pi_0$ so $\Pi(1) = \Sigma_k \Pi_k = 1$. This implies a stationary distribution exists. Since $\{a_k\}$ is a pmf, $A(1) = 1$. We want to let $t \uparrow 1$ in (3.3.9). We let $\lim_{t \uparrow 1} \frac{1 - A(t)}{1-t} = A(1) = \gamma = \Sigma_{k=0}^\infty k a_k, \gamma =$ mean number of arrivals per service interval. [Something illegible] $t \uparrow 1$ in (3.3.9), $\Pi(1) = \frac{\Pi(0)}{1-\gamma}$. We can choose $\Pi_0$ so $\Pi(1) = 1 \iff 0 < \gamma < 1$ and then $\Pi_0 = 1 - \gamma$. The queuing chain is positive recurrent $\iff \gamma < 1$, which says that the mean number of arrivals does overwhelm the facility.

Next consider $\gamma > 1$. We show that (3.3.5) has a nonzero solution $Y$ with $0 \le Y_i \le 1$ for all $i$. We choose $s = 0$, use (3.3.5) to get (3.3.10) $Y_1 = \Sigma_{i=1}^\infty a_i Y_i, \dots, Y_n = \Sigma_{i=0}^\infty a_i Y_{i+n-1}, n \ge 2$. Guessing based on branching processes, we try $Y_i = 1-t^i, 0 < t < 1$. $Y_n: 1-t^n = \Sigma_{i=0}^\infty a_i (1-t^{i+n-1}) = 1 - (\Sigma_{i=0}^\infty a_i t^i) t^{n-1}$, with $A(t) = \Sigma_{i=0}^\infty a_i t^i, t^n = A(t) t^{n-1} \Rightarrow t = A(t)$. The branching process (fixed point) analysis $\Rightarrow t = A(t)$ has a solution with $0 < t < 1$ when $\gamma > 1$. $\gamma > 1 \Rightarrow$ chain is transient. We argue that if the chain is transient, (3.3.10) has a nonzero solution. If the chain is transient, then for each $j = 0, 1, 2, \dots$, there is a last visit. There is therefore a last visit to any finite set $\{0, 1, 2, \dots, M\}$. So there is an $n_0 = n_0(M)$ such that for $n > n_0, X_n > M$. Hence, $X_n \rightarrow \infty$ as $n \rightarrow \infty$. $A_{n+1} =$ number of arrivals in $(n, n+1). P(A_{n+1} = k) = a_k, E(A_{n+1}) = p, X_{n+1} =$ max$\{ X_n - 1, 0\} + A_{n+1}, n \ge n_0, X_{n+1} = X_{n-1} + A_{n+1}$. (This is a way to get out of a low customer state when we have a low number in the queue.) If $N \ge n_0, \Sigma_{n=n_0}^N (X_{n+1} - X_n) = -(N - n_0) + \Sigma_{n=n_0}^\infty A_{n+1}, X_{N+1} - X_{n_0} = -(N - n_0) + \Sigma_{n = n_0 + 1}^{N+1} A_n, X_{N+1} - \Sigma_{n=1}^{N+1} (A_{n-1}) = X_{n_0} + n_0 - \Sigma_{n=1}^{n_0} A_n$ (entire last term up until $=$ is constant, doesn't depend on $N$). Therefore $X_{N+1} \rightarrow \infty$ implies $\Sigma_{n=1}^{N+1} (A_{n-1}) \rightarrow \infty$. Exercise: a sum of iid rv with mean $\mu$ converges to $\infty \iff \mu > 0$ equivalently $\rho > 1$.

\end{document}
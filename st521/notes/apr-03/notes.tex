\documentclass{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{amsmath}
\begin{document}

Notes - 3 April

(Review) Theorem 3.4.1 - irreducible, aperiodic MC. $P_{ij}^n \rightarrow_{n \rightarrow \infty} \frac{1}{\mu_j}$ for all $i, j$. $\mu_j$ is mean recurrent time.

Theorem 3.4.4 - An ergodic MC has both stationary and limiting distributions and these are equal.

Recurrent cases in proof: $X_n, Y_n$ have common $S, P$. $Z_n = (X_n, Y_n) \in S \times S, Z_n$ M.C. $P_{ij, kl} = P_{ik} P_{jl}$. Assume $X_n$ positive recurrent. $\Pi$ is unique stationary distribution. (With $Y_n$ ``='' $X_n$), $Z_n$ has stationary distribution $\nu = (\nu_{ij}, i, j \in S), \nu_{ij} = \Pi_i \Pi_j$. $Z_n$ is positive recurrent. Choose $X_0 = i, Y_0 = j, Z_0 = (i, j)$. Choose $s \in S$. $T =$ min$\{n \ge 1: Z_n = (s, s,)\}$. $Z_n$ recurrent $\Rightarrow P(T < \infty) = 1$.

Observation: Suppose $m \le n$ and $X_m = Y_m$. Then $X_n$ and $Y_n$ are identically distributed. Thus conditional on $\{T \le n\}, X_n$ and $Y_n$ have the same distribution. We use this observation and the fact that $T$ is finite to prove that in large time, the distributions of $X_n$ and $Y_n$ are independent of the initial values.

Computation: Start from $Z_0 = (i, j). P_{ik}^n = P(X_n=k) = P(X_n = k, T \le n) + P(X_n = k, T > n) = P(Y_n = k, T \le n) + P(Y_n = k, T > n) \le P(Y_n = k) + P(T > n) = P_{jk}^n + P(T > n)$. The symmetric argument implies $P_{jk}^n \le P_{ik}^n + P(T > n)$. Hence, $|P_{ik}^n - P_{jk}^n| \le P(T > n) \rightarrow_{n \rightarrow \infty} 0$ ($T$ finite!), for all $i, j, k \in S$. So (3.4.2) $P_{ik}^n - P_{jk}^n \rightarrow 0$ as $n \rightarrow \infty$ for all $i, j, k \in S$. If $\lim_{n \rightarrow \infty} P_{ik}^n$ exists, then it is independent of $i$. We show the limit exists. We write $\Pi_k - P_{jk}^n = \Sigma_i \Pi_i (P_{ik}^n - P_{jk}^n)$ (we can write this because $\Pi$ is stationary ($\Pi_k, P_{ik}^n$) and a pmf ($P_{jk}^n$). For any finite set $F \in S, \Sigma_{i \in S} \Pi_i |P_{ik}^n - P_{jk}^n| \le \Sigma_{i \in F} |P_{ik}^n - P_{jk}^n| + 2 \Sigma_{i \not \in F} \Pi_i$. As $n \rightarrow \infty$, this converges to $2 \Sigma_{i \not \in F} \Pi_i$. This converges to 0 as $F \uparrow S$. So (3.4.3) $\Pi_k - P_{jk}^n = \Sigma_i \Pi_i (P_{ik}^n - P_{jk}^n) \rightarrow_{n \rightarrow \infty} 0$. Read in notes about when $X_n$ is null recurrent, 185-187.

More general version of theorem 3.4.1 (no proof given) that drops irreducibility. Theorem 3.4.6 - For any aperiodic state $j$ of a Markov chain, $P_{jj}^n \rightarrow \frac{1}{\mu_j}$ as $n \rightarrow \infty$. If $i$ is any other state, $P_{ij}^n \rightarrow \frac{f_{ij}}{\mu_j}$ as $n \rightarrow \infty$. More to the theorem in notes.

$\S 3.5$ Reversibility

Some physical situations have the property that observations of the system taken at some times look the same if time runs forward or backward. Let $X_n$ be a Markov chain, $\{X_n, 0 \le n \le N\}$ irreducible, positive recurrent Markov chain, prob transition matrix $P$ and stationary distribution $\Pi$.

Definition 3.5.1 - The reversed chain or time reversal $Y_n$ is $Y_n = X_{N-n}, 0 \le n \le N$.

Theorem 3.5.1 - $Y_n$ is a Markov chain with $P(Y_{n+1} = j | Y_n = i) = \frac{\Pi_j}{\Pi_i} P_{ji}$. Proof: $P(Y_{n+1} = i_{n+1} | Y_n = i_n, \dots, Y_0 = i_0) = \frac{P(Y_k = i_j, 0 \le k \le n + 1)}{P(Y_k = i_k, 0 \le k \le n)} = \frac{P(X_{N-n-1} = i_{n+1}, X_{N-n} = i_n, \dots, X_N = i_0)}{P(X_{N-n} = i_n, \dots, X_N = i_0)} = \frac{\Pi_{i_{n+1}} P_{i_{n+1}, i_n} P_{i_n, i_{n-1}}, \dots, P_{i_1, i_0}}{\dots} = \frac{\Pi_{i_{n+1}} P_{i{n+1}, i_n}}{\Pi_{i_n}}$.

Definition 3.5.2 - The chain is reversible if the probability transition matricies of $X_n$ and its time reversal $Y_n$ are the same, (3.5.1) $\Pi_i P_{ij} = \Pi_j P_{ji}$ for all $i, j$. (3.5.1) are called the detailed balance equations. A transition matrix $P$ and a probability distribution $\lambda$ are in detailed balance if $\lambda_i P_{ij} = \lambda_j P_{ji}$ for all $i, j$. An irreducible chain $X_n$ with a stationary distribution $\Pi$ is reversible in equilibrium if its probability transition matrix is in detailed balance with $\Pi$.

Theorem 3.5.1 - Let $P$ be the probability transition matrix of an irreducible chain $X_n$ and suppose there is a distribution $\Pi$ with $\Pi_i P_{ij} = \Pi_j P_{ji}$ for all $i, j \in S$. Then $\Pi$ is the stationary distribution of $X_n$ and $X_n$ is reversible in equilibrium. Proof: $\Sigma_i \Pi_i P_{ij} = \Sigma_i \Pi_j P_{ji} = \Pi_j \Sigma_i P_{ji} = \Pi_j$ or $\Pi = \Pi P$.

\end{document}
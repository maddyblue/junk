\documentclass{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{amsmath}
\begin{document}

Notes - 24 April

Theorem 4.2.8 - Weak Markov Property - $N(t)$ is a birth process. $T$ is a fixed time. Conditional on the event $\{N(T)=i\}$ the evolution of the process  after time $T$ is independent of the evolution prior to $T$. It is useful to let $T$ be a random variable. This theorem as stated cannot hold for all possilbe random times $T$. For example, if $T$ ``looks into the future'' the conclusion cannot hold.

Definition 4.2.7 - A random time $T$ is a stopping time for the process $N(t)$ if for all $t \ge 0$, the indicator function of the event $\{T \le t\}$ is a function of the values $\{N(s), s \le t\}$ of the process up to time $t$. We can decide if $T$ has occured by $t$ by examining the values of $N$ up to time $t$.

Example 4.2.5 - The arrival times $T_1, \dots$, are stopping times. $I_{T_n = t} = \{1$ for $N(s) \ne n, N(t) = n, s < t, 0$ else. For fixed $k$, the $k^\textrm{th}$ time a process $N$ visits a state $n$ is a stopping time. The last time a process visits a state $n$ is not a stopping time.

Example 4.2.6 - $T_4 - 2$ and $\frac{1}{2} (T_1 + T_2)$ are not stopping times (why?).

For stopping times we can prove:

Theorem 4.2.9 - Strong Markov Property - Let $N(t)$ be a birth process and $T$ a stopping time. Let $A$ be an event that depends on $\{N(s), s > t\}$ and $B$ an event that depends on $\{N(s), s < t\}$. Then (4.2.9) $P(A|N \Pi) = i, B) = P(A|N(T) = i)$. Read outline of proof in notes.

$\S$ 4.3 - More on Poisson Processes

While Poisson processes are special among continuous time processes, they also occur frequently in ``nature''. Part of the reason is the Law of Rare Events.

Law of Rare Events - This says that in a situation in which a certain event can occur in any of a large number of possibilities but where the probability it occurs in any given possibility is small, then the total number of occurrences follows approximately a Poisson distribution.

Consider a large number $N$ of Bernoulli trials. The probability of success in each trial is $p$. Let $X_{N,p} =$ number of successes in the $N$ trials. (4.3.1) $P(X_{N,p} = k) = \frac{N!}{k!(N-k)!} p^k (1-p)^{N-k}, k = 0, 1, \dots, N$. We consider the limit as $N \rightarrow \infty$ and $p \rightarrow 0$ so that $\mu = Np$ is fixed. $P(X_{N,p} = k) = N(N-1) \cdots (N-k+1) \frac{p^k (1-p)^N}{k! (1-p)^k}$. Substitute $p = \frac{\mu}{N} \Rightarrow P(X_{N,p} = k) = 1 * (1 - \frac{1}{N}) \cdots (1 - \frac{k-1}{N}) \frac{\mu^k (1 - \frac{\mu}{N})^N}{k!(1-\frac{\mu}{N})^k}$. So $N \rightarrow \infty (p \rightarrow 0). 1 * (1 - \frac{1}{N}) \cdots (1 - \frac{k-1}{N}) \rightarrow 1, (1 - \frac{\mu}{N})^N \rightarrow e^{-\mu}, (1 - \frac{\mu}{N}^k) \rightarrow 1$.

Theorem 4.3.1 - Law of Rare Events - If $X_{N,p}$ is the number of successes in $N$ Bernoulli trials with probability of success $p$, then (4.3.2) $\lim_{N \rightarrow \infty, pN = \mu} P(X_{N,p} = k) = \frac{\mu^k e^{-\mu}}{k!}$. (Poisson distribution with parameter $\mu$.) (4.3.2) can be used to compute Binomial probabilities.

Suppose the probability of success varies with each trial. Let $Y_1, Y_2, \dots$, be independent Bernoulli r.v. with $P(Y_i = 1) = P_i, P(Y_i=0) = 1-P_i$. $S_n = Y_1 + \cdots + Y_N$ = number of successes in $N$ trials. $P(S_N = k) = \Sigma_{(k)} \Pi_{i=1}^N P_i^{Y_i} (1-P_i)^{N-Y_i}$ where $\Sigma_{(k)}$ is the sum over  all 0, 1 valued $Y_i$s that sum $Y_1 + \cdots + Y_N = k$.

Theorem 4.3.2 - Law of Rare Events, v.2 - (4.3.3) $|P(S_N=k) - \frac{\mu^k e^{-\mu}}{k!} | \le \Sigma_{i=1}^N P_i^2$ with $\mu = P_1 + \cdots + P_N$. If the probabilities of success in the trials are small, we obtain a good approximation. Proof: see text, pg. 285.

Recall Theorem 4.1.1 - Let $N(t)$ be a Poisson process. Then (4.1.1) $P(N(t) = j) = \frac{(\lambda t)^j}{j!} e^{-\lambda t}, j = 0, 1, \dots$ ($\lambda$ = rate of the process).

Alternate Proof (using the Law of Rare Events. Divide $[0, t]$ in to $n$ subintervals of equal length $h = \frac{t}{n}$, setting $Y_i = \{1$ if there is an event in $((i-1)\frac{t}{n}, i \frac{t}{n}], 0$ otherwise. $S_n = Y_1 + \cdots + Y_n$ counts the number of subintervals that contain at least one event and $P_i = P(Y_i = 1) = \lambda \frac{t}{n} + o(\frac{t}{n})$ by definition of a Poisson process. Using (4.3.) $|P(S_n = k) - \frac{\mu^k e^{-\mu}}{k!}| \le n (\frac{\lambda t}{n} + o(\frac{t}{n}))^2 = \frac{(\lambda t)^2}{n} + 2 \lambda + o(\frac{t}{n}) + n o((\frac{t}{n})^2)$. Now $n o(\frac{t}{n}) = t \frac{o (\frac{t}{n})}{\frac{t}{n}} = t \frac{o(h)}{h} \rightarrow_{n \rightarrow \infty, h \rightarrow 0} 0$. $S_n$ differs from $N(t)$ only if one of the subintervals has more than one event. $P(N(t) \ne S_n) = \Sigma_{i=1}^n P($number of events in $((i-1)\frac{t}{n}, i \frac{t}{n}] \ge 2 ) \le n * o (\frac{t}{n})$ by assumption. So $\lim_{n \rightarrow \infty} P(N(t) \ne S_n) = 0$.

Theorem 4.3.3 - The arrival time $T_n$ has the gamma distribution with pdf (4.3.4) $f_{T_n}(t) = \frac{\lambda^n t^{n-1}}{(n-1)!} e^{-\lambda t}, n = 1, 2, \dots, t \ge 0$. Proof: the event $\{T_n \le t\}$ occurs iff there are atleast $n$ events in $(0, t]$. $N(t)$ is a Poisson distribution with parameter mean $\lambda t$. So the cdf of $T_n$ is $F_{T_n}(t) = P(T_n \le t) = P(N(t) \ge n) = \Sigma_{k = n}^\infty \frac{(\lambda t)^k e^{-\lambda t}}{k!} = 1 - \Sigma_{k=0}^{n-1} \frac{(\lambda t)^k e^{-\lambda t}}{k!}$. Differentiating, [some long messy computation, see notes] $= \frac{\lambda^n t^{n-1} e^{-\lambda t}}{(n-1)!}$.

Theorem 4.3.4 - For $0 < s < t, 0 \le k \le n$, (4.3.5) $P(N(s) = k|N(t) = n) = \frac{n!}{k!(n-k)!} (\frac{s}{t})^k (1 - \frac{s}{t})^{n-k}$. Proof in notes.

\end{document}